{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63173b8-4913-4acb-888b-693bbe08cb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sentiment and Approval Rating: Cleaning, Training, and Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ecd3ecb-9399-4069-a9e6-cf097c048104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59394d6b-1d33-48a0-8878-78caba94f065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fetching Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac63e6a-4dde-4e1a-b4d0-d134661a9f2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark as spark\n",
    "from pyspark.sql import SparkSession\n",
    "import glob\n",
    "import os\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "import sklearn\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8511fd5-5762-4d84-8231-ed327d4ef3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e19b60-19b8-4358-9bc3-81fb4cfa6e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## approval ratings\n",
    "appr = pd.read_csv('/Workspace/Shared/approval/data/approval_ratings/approval.csv')\n",
    "\n",
    "# Get all CSV files in comments and posts folders\n",
    "comment_csv_files = glob.glob(os.path.join(\"/Workspace/Shared/approval/data/comments\", \"*.csv\"))\n",
    "posts_csv_files = glob.glob(os.path.join(\"/Workspace/Shared/approval/data/posts\", \"*.csv\"))\n",
    "\n",
    "# Read and concatenate all CSV files into respective dfs for comments and posts\n",
    "coms = pd.concat((pd.read_csv(f) for f in comment_csv_files), ignore_index=True)\n",
    "posts = pd.concat((pd.read_csv(f) for f in posts_csv_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d444a94-06e6-48ef-b1b5-e189c5a137f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(coms), len(posts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5914b106-431d-44b8-b9a7-9a5b8c2224bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Merging subreddit names onto comments section by post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08da0fbf-feeb-4510-911c-c3f1407e4b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting subreddits for each comment\n",
    "coms = coms.merge(posts[['post_id', 'subreddit']], on='post_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b96edb8-e6ea-468d-8e4d-e7fd877a2dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "coms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b495ff-ccce-4198-b3f0-e0a2f80a8557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "appr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318f979d-1010-48d5-90e0-6e9389098f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1533946-f67a-4727-9aa5-bd575fa86464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using TextBlob package for sentiment anlaysis (POS > 0, NEG < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d34ae10a-449e-4b7d-9523-5a40a9dde9d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sentiment analysis w. textblob\n",
    "coms['sentiment'] = coms['body'].apply(lambda x: TextBlob(str(x)).sentiment.polarity) # get sentiment scores\n",
    "\n",
    "# Normalize sentiment by year and subreddit\n",
    "coms['year'] = coms['created_utc'].dt.year\n",
    "yearly_scores = coms.groupby(['year', 'subreddit'])['score'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "# Multiply sentiment by normalized score\n",
    "coms['sentiment_score'] = coms['sentiment'] * yearly_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2bf5a19-e829-4a46-a103-4027e0f0c196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70d8a72-4951-4b4f-a2bd-7e369e73a29a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Date for comments data\n",
    "coms['created_utc'] = pd.to_datetime(coms['created_utc'])  # Convert to datetime\n",
    "coms['date'] = coms['created_utc'].dt.date # Creating date variable\n",
    "# finding the week it belongs to (the previous Monday) and shifting all the values by one week to better reflect the approvals\n",
    "coms['week'] = (coms['created_utc'] - pd.to_timedelta(coms['created_utc'].dt.weekday - 7, unit='d')).dt.date # Shift all values by one week and remove time\n",
    "\n",
    "# Date for approval rating data\n",
    "appr['date'] = pd.to_datetime(appr['date'])\n",
    "appr['week'] = (appr['date'] - pd.to_timedelta(appr['date'].dt.weekday, unit='d')).dt.date # Get the previous Monday and remove time\n",
    "appr['date'] = appr['date'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b79ede45-cbfe-49d5-92fc-c0612a9a9597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Restructure dataframe to:\n",
    "1) Get mean sentiment scores for each subreddit\n",
    "2) Group by date\n",
    "3) Pivot table so that (a) each feature is a subreddit, (b) each observation is a date, and (c) each cell is the mean sentiment score in that subreddit on that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c598422a-316e-4982-aaf1-9a4567e102cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by date and subreddit, calculate mean sentiment\n",
    "mean_sentiment = coms.groupby(['week', 'subreddit'])['sentiment_score'].mean().reset_index()\n",
    "\n",
    "# Pivot to make subreddits as columns\n",
    "pivoted_coms = mean_sentiment.pivot(\n",
    "    index='week',\n",
    "    columns='subreddit',\n",
    "    values='sentiment_score'\n",
    ").reset_index()\n",
    "\n",
    "# Rename the columns to clarify they're sentiment values\n",
    "pivoted_coms.columns = ['week'] + [f'sentiment_{sub}' for sub in pivoted_coms.columns[1:]]\n",
    "\n",
    "# Show result\n",
    "pivoted_coms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c606a0c-e8fa-4c8b-bc13-565abf772509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Merging pivoted comments and approval dataframes on date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f6a1db2-ec23-4e9c-bfe8-8944a6f32fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mean_approval = appr.groupby(['week', 'party'])['Approving'].mean().reset_index()\n",
    "# Merging comments and approval\n",
    "coms_appr = pd.merge(pivoted_coms, mean_approval, on='week', how='left')\n",
    "coms_appr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "886314de-7fcb-4073-9d2d-3ceb4182fe7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bce6455-1282-452c-b34e-ee59e5c92d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee4ae27-49f0-4d94-838e-9ddcfcdf1f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating dummy variable for political party\n",
    "coms_appr['party'] = coms_appr['party'].replace({'D': 0, 'R': 1})\n",
    "# Create a column for the president based on the week value\n",
    "def get_president(week):\n",
    "    if week < pd.Timestamp('2017-01-20'):\n",
    "        return 'Obama'\n",
    "    elif week < pd.Timestamp('2021-01-20'):\n",
    "        return 'Trump1'\n",
    "    elif week < pd.Timestamp('2025-01-20'):\n",
    "        return 'Biden'\n",
    "    else:\n",
    "        return 'Trump2'\n",
    "\n",
    "coms_appr['president'] = coms_appr['week'].apply(get_president)\n",
    "coms_appr['president'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ff033f-c851-4334-aa83-add23ce78d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "coms_appr[coms_appr['president']==\"Trump2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8d3622-85f9-46e5-880c-6996d3e2e0ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating pipeline to train XGBoost model\n",
    "\n",
    "*NOTE: This pipeline is programmed to work with Spark in Databricks. However, some elements are not compatible with the trial version of Databricks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d97a656-e9e9-42e9-8c03-9a42f13d7cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkxgb import XGBoostRegressor\n",
    "\n",
    "# Assume df is your preprocessed DataFrame\n",
    "label_col = \"Approving\"\n",
    "feature_cols = [col for col in coms_appr.columns if col != label_col] if not coms_appr.columns.empty else []\n",
    "\n",
    "# Assemble features into a single vectorassembler \n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# XGBoost Regressor\n",
    "xgb_reg = XGBoostRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=label_col,\n",
    "    missing=float(\"nan\"),\n",
    "    predictionCol=\"prediction\",\n",
    "    objective=\"reg:squarederror\",\n",
    "    numRound=100,\n",
    "    eta=0.1,\n",
    "    maxDepth=6,\n",
    "    nthread=4\n",
    ")\n",
    "\n",
    "# Spark Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, xgb_reg])\n",
    "\n",
    "# Fit model\n",
    "model = pipeline.fit(coms_appr)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(coms_appr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62cce439-fd85-456c-83e1-207c9669707e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "XGBoost model (no pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cafbaa3b-9ea4-4fe0-88b4-fc2cb9cf7021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "coms_appr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5cfb254-94c5-4090-ae6e-d09f7ac7e404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# XGboost model\n",
    "coms_appr = coms_appr.dropna(subset=[\"Approving\"])\n",
    "\n",
    "# Define features and target\n",
    "X = coms_appr.drop(columns=[\"Approving\", \"week\", \"president\"]) # omitting target feature and date\n",
    "y = coms_appr[\"Approving\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=10, stratify=coms_appr[\"president\"]\n",
    ")\n",
    "\n",
    "# Initialize and train the model\n",
    "model = xgb.XGBRegressor(random_state=10, missing=np.nan) # telling model to treat missing values as np.nan\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sentiment_approval_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
